{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNagCEr9VZIZ1cBlGeIpl5D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qkrjuyeol/multi-cctv/blob/main/%EC%98%81%EC%83%81_%EC%9D%B4%EC%83%81%ED%96%89%EB%8F%99_%ED%83%90%EC%A7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import xml.etree.ElementTree as ET\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models.video as video_models\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip"
      ],
      "metadata": {
        "id": "qJ1Qfb-qNBRG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 설정 ==========\n",
        "video_path = \"/content/166-1_cam01_dump02_place03_day_spring.mp4\"\n",
        "xml_path = \"/content/166-1_cam01_dump02_place03_day_spring.xml\"\n",
        "clip_output_dir = \"/content/clips\"\n",
        "clip_duration = 2  # seconds\n",
        "clip_fps = 15\n",
        "clip_length = clip_duration * clip_fps\n",
        "os.makedirs(clip_output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "eKFtxUl9NC4J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== XML 파싱 ==========\n",
        "def parse_drop_ranges(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    drop_ranges = []\n",
        "    for obj in root.findall(\"object\"):\n",
        "        action = obj.find(\"action\")\n",
        "        if action is not None and action.find(\"actionname\").text == \"drop\":\n",
        "            start = int(action.find(\"frame/start\").text)\n",
        "            end = int(action.find(\"frame/end\").text)\n",
        "            drop_ranges.append((start, end))\n",
        "    return drop_ranges"
      ],
      "metadata": {
        "id": "WJbr3_VbNGoB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 클립 추출 ==========\n",
        "def extract_clips_from_video(video_path, drop_ranges, out_dir, fps=30, clip_len=clip_length):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    width = 224\n",
        "    height = 224\n",
        "\n",
        "    def save_clip(frames, out_path):\n",
        "        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), clip_fps, (width, height))\n",
        "        for f in frames:\n",
        "            resized = cv2.resize(f, (width, height))\n",
        "            out.write(resized)\n",
        "        out.release()\n",
        "\n",
        "    clip_id = 0\n",
        "    for i in range(0, total_frames - clip_len, clip_len):\n",
        "        label = \"normal\"\n",
        "        for start, end in drop_ranges:\n",
        "            if i >= start and i + clip_len <= end:\n",
        "                label = \"drop\"\n",
        "                break\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        frames = []\n",
        "        for _ in range(clip_len):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frames.append(frame)\n",
        "        if len(frames) == clip_len:\n",
        "            save_path = os.path.join(out_dir, f\"{label}_{clip_id}.mp4\")\n",
        "            save_clip(frames, save_path)\n",
        "            clip_id += 1\n",
        "    cap.release()"
      ],
      "metadata": {
        "id": "CRnPY9tMNJGc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== PyTorch Dataset ==========\n",
        "class VideoClipDataset(Dataset):\n",
        "    def __init__(self, clip_dir, clip_len=clip_length, transform=None):\n",
        "        self.paths = glob(os.path.join(clip_dir, \"*.mp4\"))\n",
        "        self.labels = [1 if \"drop\" in p else 0 for p in self.paths]\n",
        "        self.transform = transform\n",
        "        self.clip_len = clip_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frames = []\n",
        "        for _ in range(self.clip_len):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, (112, 112))\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "        frames = np.stack(frames)\n",
        "        frames = frames.transpose(3, 0, 1, 2)  # (C, T, H, W)\n",
        "        frames = torch.tensor(frames, dtype=torch.float32) / 255.0\n",
        "        if self.transform:\n",
        "            frames = self.transform(frames)\n",
        "        return frames, torch.tensor(label)"
      ],
      "metadata": {
        "id": "3n08gXFpNLVD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 모델 구성 (3D CNN - torchvision resnet18 기반) ==========\n",
        "def build_model():\n",
        "    model = video_models.r3d_18(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-VIp83P_NNIJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 학습 ==========\n",
        "def train(model, dataloader, device, epochs=5):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss, total_acc = 0, 0\n",
        "        for x, y in tqdm(dataloader):\n",
        "            x, y = x.to(device), y.float().to(device)\n",
        "            logits = model(x).squeeze()\n",
        "            loss = criterion(logits, y)\n",
        "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "            acc = (preds == y).float().mean()\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}: Loss {total_loss/len(dataloader):.4f}, Acc {total_acc/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "ZknlqHptNO05"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 실행 ==========\n",
        "drop_ranges = parse_drop_ranges(xml_path)\n",
        "extract_clips_from_video(video_path, drop_ranges, clip_output_dir)\n",
        "\n",
        "dataset = VideoClipDataset(clip_output_dir)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = build_model()\n",
        "train(model, dataloader, device, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsn5fBiONQrT",
        "outputId": "b64b466d-bd38-4295-ee31-df2af2917091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 75/75 [56:11<00:00, 44.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss 0.0752, Acc 0.9900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [56:00<00:00, 44.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss 0.0410, Acc 0.9933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [55:50<00:00, 44.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss 0.0557, Acc 0.9900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 54/75 [40:01<15:30, 44.31s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 모델 저장 ==========\n",
        "torch.save(model.state_dict(), \"/content/dump_detection_r3d18.pth\")\n",
        "print(\"저장 완료!\")\n"
      ],
      "metadata": {
        "id": "G1JiwKBxM5wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_model(model, test_dataset, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    for i in range(5):  # 앞에서 5개만 확인해보기\n",
        "        clip, label = test_dataset[i]\n",
        "        input_tensor = clip.unsqueeze(0).to(device)  # (1, C, T, H, W)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_tensor).squeeze()\n",
        "            prob = torch.sigmoid(logits).item()\n",
        "            pred_label = 1 if prob > 0.5 else 0\n",
        "\n",
        "        true_label = int(label.item())\n",
        "        print(f\"[Clip {i}] GT: {true_label} | Pred: {pred_label} | Confidence: {prob:.4f}\")\n",
        "\n",
        "        # 첫 프레임 시각화\n",
        "        frames = clip.permute(1, 2, 3, 0).numpy()  # (T, H, W, C)\n",
        "        plt.imshow(frames[0])\n",
        "        plt.title(f\"GT: {'DROP' if true_label else 'NORMAL'} | Pred: {'DROP' if pred_label else 'NORMAL'}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# 실행\n",
        "test_model(model, dataset, device)"
      ],
      "metadata": {
        "id": "fxqanLpr8zp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
