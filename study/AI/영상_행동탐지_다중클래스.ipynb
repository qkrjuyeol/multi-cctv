{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_video\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torchvision.models import video as video_models\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class VideoClipDataset(Dataset):\n",
        "    def __init__(self, root_dir, classes, clip_len=60):\n",
        "        self.root_dir = root_dir\n",
        "        self.classes = classes\n",
        "        self.clip_len = clip_len\n",
        "        self.samples = []\n",
        "        for label_idx, label in enumerate(classes):\n",
        "            class_dir = os.path.join(root_dir, label)\n",
        "            for fname in os.listdir(class_dir):\n",
        "                if fname.endswith(\".mp4\"):\n",
        "                    self.samples.append((os.path.join(class_dir, fname), label_idx))\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((112, 112)),\n",
        "            transforms.Normalize([0.43216, 0.394666, 0.37645],\n",
        "                                 [0.22803, 0.22145, 0.216989])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, label = self.samples[idx]\n",
        "        video, _, _ = read_video(video_path, pts_unit='sec')  # (T, H, W, C)\n",
        "\n",
        "        video = video.permute(0, 3, 1, 2).float() / 255.0  # (T, C, H, W)\n",
        "\n",
        "        if video.shape[0] > self.clip_len:\n",
        "            video = video[:self.clip_len]\n",
        "        elif video.shape[0] < self.clip_len:\n",
        "            pad = self.clip_len - video.shape[0]\n",
        "            video = torch.cat([video, video[-1:].repeat(pad, 1, 1, 1)], dim=0)\n",
        "\n",
        "        if self.transform:\n",
        "            video = self.transform(video)\n",
        "\n",
        "        video = video.permute(1, 0, 2, 3)  # (C, T, H, W) ← 여기 추가!\n",
        "\n",
        "        return video, label\n",
        "\n",
        "def build_model(num_classes=6):\n",
        "    model = video_models.r3d_18(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def train(model, dataloader, device, epochs=5):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss, total_acc = 0, 0\n",
        "        for x, y in tqdm(dataloader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            acc = (preds == y).float().mean()\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}: Loss {total_loss/len(dataloader):.4f}, Acc {total_acc/len(dataloader):.4f}\")\n",
        "\n",
        "classes = ['normal', 'trespass', 'fight', 'dump', 'burglary', 'vandalism']\n",
        "clip_output_dir = r\"D:\\clips\"\n",
        "\n",
        "dataset = VideoClipDataset(clip_output_dir, classes)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = build_model(num_classes=len(classes))\n",
        "train(model, dataloader, device, epochs=10)\n"
      ],
      "metadata": {
        "id": "UClRAC2LnQ_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}